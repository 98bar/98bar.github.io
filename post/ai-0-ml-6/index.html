<!DOCTYPE html>
<html lang="zh-CN">


<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1.0, user-scalable=no">
  <meta name="theme-color" content="#202020"/>
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  
    <meta name="keywords" content="机器学习,深度学习,强化学习," />
  

  
    <meta name="description" content="QH的博客" />
  
  
  
  <link rel="icon" type="image/x-icon" href="/images/favicon.ico">
  
  <title>机器学习系列（六）K近邻算法 - QH的博客</title>
  
    <!-- stylesheets list from config.yml -->
    
      <link rel="stylesheet" href="/css/pure-min.css">
    
      <link rel="stylesheet" href="/css/xoxo.css">
    
  
<meta name="generator" content="Hexo 4.2.1"><link rel="alternate" href="/atom.xml" title="QH的博客" type="application/atom+xml">
</head>


<body>
<div class="nav-container">
<nav class="home-menu pure-menu pure-menu-horizontal">
  <a class="pure-menu-heading" href="/">
    
      <img class="avatar" src="https://smiler666.github.io/images/favicon.ico">
    
    <span class="title" style="text-transform:none">QH的博客</span>
  </a>

  <ul class="pure-menu-list clearfix">
      
          
            
              <li class="pure-menu-item"><a href="/" class="pure-menu-link">首页</a></li>
            
          
      
          
            
              <li class="pure-menu-item"><a href="/archives" class="pure-menu-link">文章</a></li>
            
          
      
          
            
              <li class="pure-menu-item"><a href="/post/share-1/" class="pure-menu-link">工具</a></li>
            
          
      
          
            
              <li class="pure-menu-item"><a href="/about" class="pure-menu-link">关于</a></li>
            
          
      
          
            
              <li class="pure-menu-item"><a href="/search" class="pure-menu-link">搜索</a></li>
            
          
      
  </ul>
   
</nav>

</div>

<div class="container" id="content-outer">
<div class="inner" id="content-inner">
<div class="post-container">
  <article class="post" id="post">
    <header class="post-header text-center">
      <h1 class="title">
        机器学习系列（六）K近邻算法
      </h1>
      <span>
        
        <time class="time" datetime="2021-03-27T04:20:00.000Z">
        2021-03-27
      </time>
        
      </span>
      <span class="slash">/</span>
      <span class="post-meta">
      <span class="post-tags">
        <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" rel="tag">强化学习</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag">深度学习</a></li></ul>
      </span>
    </span>
      <span class="slash">/</span>
      <span class="read">阅读耗时 9 分钟</span>
    </header>

    <div class="post-content">
      <a id="more"></a>
<h1 id="第6章：K近邻算法简介"><a href="#第6章：K近邻算法简介" class="headerlink" title="第6章：K近邻算法简介"></a>第6章：K近邻算法简介</h1><p>K近邻（K-nearest neighbors）是一种简单而强大的监督学习算法，常用于分类和回归问题。K近邻算法的基本思想是根据已有的数据集，通过计算新样本与邻近样本之间的距离，从而确定新样本的类别或数值。</p>
<h2 id="6-1-算法简介"><a href="#6-1-算法简介" class="headerlink" title="6.1 算法简介"></a>6.1 算法简介</h2><p>K近邻算法的基本步骤如下： </p>
<ol>
<li><strong>数据准备</strong> ：收集和整理用于训练和测试的数据集，包括特征（输入）和标签（输出）。 </li>
<li><strong>距离度量</strong> ：选择适合的距离度量方法，例如欧氏距离、曼哈顿距离等，用于计算样本之间的相似度。 </li>
<li><strong>K值选择</strong> ：选择一个合适的K值，表示要考虑的邻近样本的数量。 </li>
<li><strong>邻近样本选择</strong> ：根据选定的距离度量方法，计算新样本与训练集中所有样本之间的距离，并选择距离最近的K个样本作为邻近样本。 </li>
<li><strong>投票决策</strong> ：对于分类问题，根据邻近样本的标签，通过投票的方式决定新样本的类别；对于回归问题，根据邻近样本的数值，通过平均或加权平均的方式估计新样本的数值。</li>
</ol>
<p>K近邻算法中常用的距离度量公式如下： </p>
<ul>
<li><p>欧氏距离：</p>
<script type="math/tex; mode=display">
d(x,y) = ∑_{i=1}^{n} (x_i - y_i)^2</script></li>
<li><p>曼哈顿距离：</p>
<script type="math/tex; mode=display">
d(x,y) = ∑_{i=1}^{n} |x_i - y_i|</script></li>
</ul>
<p>其中，$x$和$y$分别表示两个样本的特征向量，$n$表示特征的维度。</p>
<h2 id="6-2-例子"><a href="#6-2-例子" class="headerlink" title="6.2 例子"></a>6.2 例子</h2><p>下面以一个简单的分类问题为例，使用K近邻算法进行分类。</p>
<h3 id="6-2-1-导入数据集"><a href="#6-2-1-导入数据集" class="headerlink" title="6.2.1 导入数据集"></a>6.2.1 导入数据集</h3><p>假设有一个鸢尾花数据集，包含150个样本，每个样本有4个特征：花萼长度、花萼宽度、花瓣长度、花瓣宽度，以及一个标签：鸢尾花的种类（setosa、versicolor、virginica）。我们希望根据已有的样本数据，对新样本进行分类。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载鸢尾花数据集</span></span><br><span class="line">iris = load_iris()</span><br><span class="line">X = iris.data</span><br><span class="line">y = iris.target</span><br></pre></td></tr></table></figure>
<h3 id="6-2-2-数据预处理"><a href="#6-2-2-数据预处理" class="headerlink" title="6.2.2 数据预处理"></a>6.2.2 数据预处理</h3><p>对数据进行预处理，包括将数据集划分为训练集和测试集，以及进行特征缩放。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 划分数据集为训练集和测试集</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 特征缩放（可选）</span></span><br><span class="line"><span class="comment"># 这里可以使用不同的方法对特征进行缩放，例如MinMaxScaler、StandardScaler等</span></span><br><span class="line"><span class="comment"># 这里我们不进行特征缩放，因为鸢尾花数据集的特征已经在相似的数值范围内</span></span><br></pre></td></tr></table></figure>
<h3 id="6-2-3-K近邻算法实现"><a href="#6-2-3-K近邻算法实现" class="headerlink" title="6.2.3 K近邻算法实现"></a>6.2.3 K近邻算法实现</h3><p>接下来，我们可以使用scikit-learn库中的KNeighborsClassifier类来实现K近邻算法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建K近邻分类器</span></span><br><span class="line">k = <span class="number">3</span>  <span class="comment"># 设置K值</span></span><br><span class="line">clf = KNeighborsClassifier(n_neighbors=k)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在训练集上训练K近邻分类器</span></span><br><span class="line">clf.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在测试集上进行预测</span></span><br><span class="line">y_pred = clf.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算预测准确率</span></span><br><span class="line">accuracy = accuracy_score(y_test, y_pred)</span><br><span class="line">print(<span class="string">"Accuracy:"</span>, accuracy)</span><br></pre></td></tr></table></figure>
<h3 id="6-2-4-结果解释"><a href="#6-2-4-结果解释" class="headerlink" title="6.2.4 结果解释"></a>6.2.4 结果解释</h3><p>最后，我们可以根据预测结果进行解释和分析。例如，可以输出混淆矩阵、分类报告等来评估分类器的性能。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix, classification_report</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出混淆矩阵和分类报告</span></span><br><span class="line">cm = confusion_matrix(y_test, y_pred)</span><br><span class="line">cr = classification_report(y_test, y_pred, target_names=iris.target_names)</span><br><span class="line">print(<span class="string">"Confusion Matrix:"</span>)</span><br><span class="line">print(cm)</span><br><span class="line">print(<span class="string">"\nClassification Report:"</span>)</span><br><span class="line">print(cr)</span><br></pre></td></tr></table></figure>
<h2 id="6-3-总结"><a href="#6-3-总结" class="headerlink" title="6.3 总结"></a>6.3 总结</h2><p>本章介绍了K近邻算法的基本概念和应用。K近邻算法是一种简单但有效的监督学习方法，常用于分类和回归问题。算法的主要思想是通过计算样本之间的距离来进行预测或分类，其中K值是一个重要的参数，决定了邻居的数量。本章通过一个实例介绍了K近邻算法的基本步骤，包括数据预处理、K近邻算法的实现以及结果解释和分析。</p>
<p>在使用K近邻算法时，需要注意的几点是：</p>
<ul>
<li>数据预处理：包括将数据集划分为训练集和测试集，并进行必要的特征缩放等处理。</li>
<li>K值的选择：K值的选择对算法的性能有很大影响，需要根据具体问题和数据集进行调优。</li>
<li>距离度量：K近邻算法使用距离度量来计算样本之间的相似度，常用的距离度量有欧氏距离和曼哈顿距离等。</li>
<li>结果解释和分析：在使用K近邻算法进行预测或分类后，需要对结果进行解释和分析，包括混淆矩阵、分类报告等评估指标。</li>
</ul>
<p>虽然K近邻算法简单易懂，但也存在一些局限性，例如对于高维数据和大规模数据集的处理可能会受到限制。因此，在使用K近邻算法时，需要结合具体问题和数据集的特点来进行合理的选择和调优。希望本章的介绍能够帮助初学者理解K近邻算法的基本原理和应用，并在实际问题中进行实践和应用。</p>

    </div>

  </article>
  <div class="toc-container">
    
  <div id="toc" class="toc-article">
    <strong class="toc-title">目录</strong>
    <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#第6章：K近邻算法简介"><span class="toc-text">第6章：K近邻算法简介</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#6-1-算法简介"><span class="toc-text">6.1 算法简介</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-2-例子"><span class="toc-text">6.2 例子</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2-1-导入数据集"><span class="toc-text">6.2.1 导入数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2-2-数据预处理"><span class="toc-text">6.2.2 数据预处理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2-3-K近邻算法实现"><span class="toc-text">6.2.3 K近邻算法实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2-4-结果解释"><span class="toc-text">6.2.4 结果解释</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-3-总结"><span class="toc-text">6.3 总结</span></a></li></ol></li></ol>
  </div>


  </div>
</div>
<div class="copyright">
    <span>本作品采用</span>
    <a href="https://creativecommons.org/licenses/by-nc-nd/4.0/deed.zh" target="_blank" rel="noopener">署名-非商业性使用-禁止演绎 4.0 国际许可协议</a>
    <span>进行许可。转载时请注明原文链接。</span>
</div>


  
    <div class="post-nav">
      <div class="post-nav-item post-nav-next">
        
          <span>〈 </span>
          <a href="/post/ai-0-ml-5/" rel="next" title="机器学习系列（五）贝叶斯算法">
          机器学习系列（五）贝叶斯算法
          </a>
        
      </div>
  
      <div class="post-nav-item post-nav-prev">
          
          <a href="/post/ai-0-ml-7/" rel="prev" title="机器学习系列（七）支持向量机">
            机器学习系列（七）支持向量机
          </a>
          <span>〉</span>
        
      </div>
    </div>
  



</div>
</div>

<footer class="footer text-center">
    <div id="bottom-inner">
        <a class="bottom-item" href="" target="_blank">QH的博客</a> |
        <a class="bottom-item">Powered by hexo</a> |
        <a class="bottom-item">Copyright © 2023</a>  |
        <a>博客已运行<a id="days">0</a>天</a> 
        <script> 
        var s1 = '2020-04-15';//设置为你的建站时间 
        s1 = new Date(s1.replace(/-/g, "/")); 
        s2 = new Date(); 
        var days = s2.getTime() - s1.getTime(); 
        var number_of_days = parseInt(days / (1000 * 60 * 60 * 24)); 
        document.getElementById('days').innerHTML = number_of_days; 
        </script> |
        <a class="bottom-item" href="/atom.xml">RSS</a>
        <script>
			var _hmt = _hmt || [];
			(function() {
			  var hm = document.createElement("script");
			  hm.src = "https://hm.baidu.com/hm.js?7241c66da4c19bc02e939e6776919ff8";
			  var s = document.getElementsByTagName("script")[0]; 
			  s.parentNode.insertBefore(hm, s);
			})();
		</script>
    </div>
</footer>



<script>
  (function(window, document, undefined) {

    var timer = null;

    function returnTop() {
      cancelAnimationFrame(timer);
      timer = requestAnimationFrame(function fn() {
        var oTop = document.body.scrollTop || document.documentElement.scrollTop;
        if (oTop > 0) {
          document.body.scrollTop = document.documentElement.scrollTop = oTop - 50;
          timer = requestAnimationFrame(fn);
        } else {
          cancelAnimationFrame(timer);
        }
      });
    }

    var hearts = [];
    window.requestAnimationFrame = (function() {
      return window.requestAnimationFrame ||
        window.webkitRequestAnimationFrame ||
        window.mozRequestAnimationFrame ||
        window.oRequestAnimationFrame ||
        window.msRequestAnimationFrame ||
        function(callback) {
          setTimeout(callback, 1000 / 60);
        }
    })();
    init();

    function init() {
      css(".heart{z-index:9999;width: 10px;height: 10px;position: fixed;background: #f00;transform: rotate(45deg);-webkit-transform: rotate(45deg);-moz-transform: rotate(45deg);}.heart:after,.heart:before{content: '';width: inherit;height: inherit;background: inherit;border-radius: 50%;-webkit-border-radius: 50%;-moz-border-radius: 50%;position: absolute;}.heart:after{top: -5px;}.heart:before{left: -5px;}");
      attachEvent();
      gameloop();
      addMenuEvent();
    }

    function gameloop() {
      for (var i = 0; i < hearts.length; i++) {
        if (hearts[i].alpha <= 0) {
          document.body.removeChild(hearts[i].el);
          hearts.splice(i, 1);
          continue;
        }
        hearts[i].y--;
        hearts[i].scale += 0.004;
        hearts[i].alpha -= 0.013;
        hearts[i].el.style.cssText = "left:" + hearts[i].x + "px;top:" + hearts[i].y + "px;opacity:" + hearts[i].alpha + ";transform:scale(" + hearts[i].scale + "," + hearts[i].scale + ") rotate(45deg);background:" + hearts[i].color;
      }
      requestAnimationFrame(gameloop);
    }

    /**
     * 给logo设置点击事件
     * 
     * - 回到顶部
     * - 出现爱心
     */
    function attachEvent() {
      var old = typeof window.onclick === "function" && window.onclick;
      var logo = document.getElementById("logo");
      if (logo) {
        logo.onclick = function(event) {
          returnTop();
          old && old();
          createHeart(event);
        }
      }
      
    }

    function createHeart(event) {
      var d = document.createElement("div");
      d.className = "heart";
      hearts.push({
        el: d,
        x: event.clientX - 5,
        y: event.clientY - 5,
        scale: 1,
        alpha: 1,
        color: randomColor()
      });
      document.body.appendChild(d);
    }

    function css(css) {
      var style = document.createElement("style");
      style.type = "text/css";
      try {
        style.appendChild(document.createTextNode(css));
      } catch (ex) {
        style.styleSheet.cssText = css;
      }
      document.getElementsByTagName('head')[0].appendChild(style);
    }

    function randomColor() {
      // return "rgb(" + (~~(Math.random() * 255)) + "," + (~~(Math.random() * 255)) + "," + (~~(Math.random() * 255)) + ")";
      return "#F44336";
    }

    function addMenuEvent() {
      var menu = document.getElementById('menu-main-post');
      if (menu) {
        var toc = document.getElementById('toc');
        if (toc) {
          menu.onclick = function() {
            if (toc) {
              if (toc.style.display == 'block') {
                toc.style.display = 'none';
              } else {
                toc.style.display = 'block';
              }
            }
          };
        } else {
          menu.style.display = 'none';
        }
      }
    }

  })(window, document);
</script>






<script>
  (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      if (curProtocol === 'https') {
          bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
      }
      else {
          bp.src = 'http://push.zhanzhang.baidu.com/push.js';
      }
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
  })();
</script>


<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
</body>
</html>