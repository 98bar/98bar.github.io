<!DOCTYPE html>
<html lang="zh-CN">


<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1.0, user-scalable=no">
  <meta name="theme-color" content="#202020"/>
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  
    <meta name="keywords" content="机器学习,深度学习,强化学习," />
  

  
    <meta name="description" content="QH的博客" />
  
  
  
  <link rel="icon" type="image/x-icon" href="/images/favicon.ico">
  
  <title>机器学习系列（十）无监督学习之降维 - QH的博客</title>
  
    <!-- stylesheets list from config.yml -->
    
      <link rel="stylesheet" href="/css/pure-min.css">
    
      <link rel="stylesheet" href="/css/xoxo.css">
    
  
<meta name="generator" content="Hexo 4.2.1"><link rel="alternate" href="/atom.xml" title="QH的博客" type="application/atom+xml">
</head>


<body>
<div class="nav-container">
<nav class="home-menu pure-menu pure-menu-horizontal">
  <a class="pure-menu-heading" href="/">
    
      <img class="avatar" src="https://smiler666.github.io/images/favicon.ico">
    
    <span class="title" style="text-transform:none">QH的博客</span>
  </a>

  <ul class="pure-menu-list clearfix">
      
          
            
              <li class="pure-menu-item"><a href="/" class="pure-menu-link">首页</a></li>
            
          
      
          
            
              <li class="pure-menu-item"><a href="/archives" class="pure-menu-link">文章</a></li>
            
          
      
          
            
              <li class="pure-menu-item"><a href="/about" class="pure-menu-link">关于</a></li>
            
          
      
          
            
              <li class="pure-menu-item"><a href="/search" class="pure-menu-link">搜索</a></li>
            
          
      
  </ul>
   
</nav>

</div>

<div class="container" id="content-outer">
<div class="inner" id="content-inner">
<div class="post-container">
  <article class="post" id="post">
    <header class="post-header text-center">
      <h1 class="title">
        机器学习系列（十）无监督学习之降维
      </h1>
      <span>
        
        <time class="time" datetime="2021-03-31T04:20:00.000Z">
        2021-03-31
      </time>
        
      </span>
      <span class="slash">/</span>
      <span class="post-meta">
      <span class="post-tags">
        <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" rel="tag">强化学习</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag">深度学习</a></li></ul>
      </span>
    </span>
      <span class="slash">/</span>
      <span class="read">阅读耗时 16 分钟</span>
    </header>

    <div class="post-content">
      <a id="more"></a>
<h1 id="第10章：无监督学习之降维"><a href="#第10章：无监督学习之降维" class="headerlink" title="第10章：无监督学习之降维"></a>第10章：无监督学习之降维</h1><p>当涉及到机器学习中的无监督学习降维时，常用的方法有主成分分析（PCA）、独立成分分析（ICA）、 t-分布随机近邻嵌入（t-SNE）和自编码器（Autoencoder）等。下面将分别对这些方法进行简单介绍，并给出相应的 Python 代码示例。</p>
<h2 id="10-1-主成分分析（PCA）"><a href="#10-1-主成分分析（PCA）" class="headerlink" title="10.1 主成分分析（PCA）"></a>10.1 主成分分析（PCA）</h2><h3 id="10-1-1-简介"><a href="#10-1-1-简介" class="headerlink" title="10.1.1 简介"></a>10.1.1 简介</h3><p>主成分分析（Principal Component Analysis，简称 PCA）是一种常用的无监督学习降维方法，通过线性变换将原始高维数据映射到低维空间，使得映射后的数据保留了原始数据中的主要信息。PCA 的核心思想是找到数据中的主成分（即数据中方差最大的方向），然后将数据投影到这些主成分上，从而实现降维。</p>
<h3 id="10-1-2-算法"><a href="#10-1-2-算法" class="headerlink" title="10.1.2 算法"></a>10.1.2 算法</h3><ol>
<li>对所有样本进行中心化处理。</li>
<li>计算样本协方差矩阵。</li>
<li>对协方差矩阵进行特征值分解。</li>
<li>选取前k个最大特征值所对应的特征向量作为新的坐标系。</li>
<li>将原始数据投影到新坐标系中。</li>
</ol>
<h3 id="10-1-3-公式"><a href="#10-1-3-公式" class="headerlink" title="10.1.3 公式"></a>10.1.3 公式</h3><ol>
<li>样本协方差矩阵：$S=\frac{1}{n-1}\sum_{i=1}{n}(x_i-\bar{x})(x_i-\bar{x})T$</li>
<li>特征值和特征向量：$Sv=\lambda v$</li>
</ol>
<h3 id="10-1-4-代码"><a href="#10-1-4-代码" class="headerlink" title="10.1.4 代码"></a>10.1.4 代码</h3><p>假设有一个二维数据集，其中每个样本有两个特征：$x_1$和$x_2$。想要将这个数据集降到一维。下面是Python代码实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成二维数据集</span></span><br><span class="line">X = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 中心化处理</span></span><br><span class="line">X_mean = np.mean(X, axis=<span class="number">0</span>)</span><br><span class="line">X_centered = X - X_mean</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算协方差矩阵</span></span><br><span class="line">S = np.cov(X_centered.T)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 特征值分解</span></span><br><span class="line">eigenvalues, eigenvectors = np.linalg.eig(S)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 取最大特征值对应的特征向量作为新坐标系</span></span><br><span class="line">w = eigenvectors[:, np.argmax(eigenvalues)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 投影到新坐标系中</span></span><br><span class="line">X_pca = X_centered.dot(w)</span><br></pre></td></tr></table></figure>
<p>以下是使用 Scikit-learn 库中的 PCA 类进行 PCA 降维的示例代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成示例数据</span></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line">X = np.random.randn(<span class="number">100</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化 PCA 模型</span></span><br><span class="line">pca = PCA(n_components=<span class="number">1</span>)  <span class="comment"># 设置降维后的维度为1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 对数据进行降维</span></span><br><span class="line">X_pca = pca.fit_transform(X)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制降维后的数据</span></span><br><span class="line">plt.scatter(X_pca, np.zeros_like(X_pca), alpha=<span class="number">0.7</span>)</span><br><span class="line">plt.xlabel(<span class="string">'PC1'</span>)</span><br><span class="line">plt.ylabel(<span class="string">''</span>)</span><br><span class="line">plt.title(<span class="string">'PCA降维示例'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h2 id="10-2-独立成分分析（ICA）"><a href="#10-2-独立成分分析（ICA）" class="headerlink" title="10.2 独立成分分析（ICA）"></a>10.2 独立成分分析（ICA）</h2><h3 id="10-2-1-简介"><a href="#10-2-1-简介" class="headerlink" title="10.2.1 简介"></a>10.2.1 简介</h3><p>独立成分分析（Independent Component Analysis，简称 ICA）是一种基于概率统计的无监督学习降维方法，它通过寻找数据中的独立成分（即在概率分布上相互独立的成分）来实现降维。ICA 在信号处理、图像处理等领域中有广泛的应用。</p>
<h3 id="10-2-2-算法"><a href="#10-2-2-算法" class="headerlink" title="10.2.2 算法"></a>10.2.2 算法</h3><ol>
<li>对所有样本进行中心化处理。</li>
<li>随机初始化权重向量。</li>
<li>对权重向量进行更新，使得输出信号的熵最大化。</li>
<li>重复步骤3直到收敛。</li>
</ol>
<h3 id="10-2-3-公式"><a href="#10-2-3-公式" class="headerlink" title="10.2.3 公式"></a>10.2.3 公式</h3><ol>
<li>输出信号：$y=As$</li>
<li>目标函数：$J(w)=\sum_{i=1}{n}(log(cosh(wTx_i)))$</li>
<li>权重向量更新规则：$w=w+\alpha E[wTx_i*tanh(wTx_i)-g(w)]$</li>
</ol>
<h3 id="10-2-4-代码"><a href="#10-2-4-代码" class="headerlink" title="10.2.4 代码"></a>10.2.4 代码</h3><p>假设我有一个二维数据集，其中每个样本有两个特征：$x_1$和$x_2$。想要将这个数据集分离成两个独立的信号源。下面是Python代码实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成二维数据集</span></span><br><span class="line">X = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 中心化处理</span></span><br><span class="line">X_mean = np.mean(X, axis=<span class="number">0</span>)</span><br><span class="line">X_centered = X - X_mean</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机初始化权重向量</span></span><br><span class="line">w = np.random.rand(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 更新权重向量</span></span><br><span class="line">alpha = <span class="number">0.01</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    y = X_centered.dot(w)</span><br><span class="line">    g = np.tanh(y)</span><br><span class="line">    w += alpha * (X_centered.T.dot(g) - np.mean(<span class="number">1</span> - g**<span class="number">2</span>) * w)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分离信号源</span></span><br><span class="line">S = X_centered.dot(w)</span><br></pre></td></tr></table></figure>
<p>以下是使用 Scikit-learn 库中的 FastICA 类进行 ICA 降维的示例代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> FastICA</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成示例数据</span></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line">X = np.random.randn(<span class="number">100</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化 ICA 模型</span></span><br><span class="line">ica = FastICA(n_components=<span class="number">1</span>)  <span class="comment"># 设置降维后的维度为1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 对数据进行降维</span></span><br><span class="line">X_ica = ica.fit_transform(X)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制降维后的数据</span></span><br><span class="line">plt.scatter(X_ica, np.zeros_like(X_ica), alpha=<span class="number">0.7</span>)</span><br><span class="line">plt.xlabel(<span class="string">'IC1'</span>)</span><br><span class="line">plt.ylabel(<span class="string">''</span>)</span><br><span class="line">plt.title(<span class="string">'ICA降维示例'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h2 id="10-3-t-分布随机近邻嵌入（t-SNE）"><a href="#10-3-t-分布随机近邻嵌入（t-SNE）" class="headerlink" title="10.3 t-分布随机近邻嵌入（t-SNE）"></a>10.3 t-分布随机近邻嵌入（t-SNE）</h2><h3 id="10-3-1-简介"><a href="#10-3-1-简介" class="headerlink" title="10.3.1 简介"></a>10.3.1 简介</h3><p>t-分布随机近邻嵌入（t-Distributed Stochastic Neighbor Embedding，简称t-SNE）是一种非线性的无监督学习降维方法，用于可视化高维数据。t-SNE 通过在高维空间中找到数据点之间的相似性，并将其映射到低维空间中，从而实现降维。t-SNE 通常在数据可视化和聚类等领域中被广泛应用。</p>
<h3 id="10-3-2-算法"><a href="#10-3-2-算法" class="headerlink" title="10.3.2 算法"></a>10.3.2 算法</h3><ol>
<li>计算高维空间中每个点之间的相似度。</li>
<li>随机初始化低维空间中每个点的位置。</li>
<li>计算低维空间中每个点之间的相似度。</li>
<li>计算高维空间和低维空间之间的相似度。</li>
<li>最小化高维空间和低维空间之间相似度的KL散度。</li>
</ol>
<h3 id="10-3-3-公式"><a href="#10-3-3-公式" class="headerlink" title="10.3.3 公式"></a>10.3.3 公式</h3><ol>
<li>高维空间中两个点$i$和$j$之间的相似度：$p_{j|i}=\frac{exp(-||x_i-x_j||2/2\sigma_i2)}{\sum_{k\neq i}exp(-||x_i-x_k||2/2\sigma_i2)}$</li>
<li>低维空间中两个点$i$和$j$之间的相似度：$q_{j|i}=\frac{(1+||y_i-y_j||2){-1}}{\sum_{k\neq i}(1+||y_i-y_k||2){-1}}$</li>
<li>KL散度：$KL(P||Q)=\sum_{i,j}p_{j|i}log\frac{p_{j|i}}{q_{j|i}}$</li>
</ol>
<h3 id="10-3-4-代码"><a href="#10-3-4-代码" class="headerlink" title="10.3.4 代码"></a>10.3.4 代码</h3><p>假设有一个二维数据集，其中每个样本有两个特征：$x_1$和$x_2$。想要将这个数据集降到二维并进行可视化。下面是Python代码实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_blobs</span><br><span class="line"><span class="keyword">from</span> sklearn.manifold <span class="keyword">import</span> TSNE</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成二维数据集</span></span><br><span class="line">X, y = make_blobs(n_samples=<span class="number">1000</span>, centers=<span class="number">3</span>, n_features=<span class="number">2</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># t-SNE降维</span></span><br><span class="line">tsne = TSNE(n_components=<span class="number">2</span>, perplexity=<span class="number">30</span>, learning_rate=<span class="number">200</span>)</span><br><span class="line">X_tsne = tsne.fit_transform(X)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化结果</span></span><br><span class="line">plt.scatter(X_tsne[:, <span class="number">0</span>], X_tsne[:, <span class="number">1</span>], c=y)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>以下是使用 Scikit-learn 库中的 TSNE 类进行 t-SNE 降维的示例代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.manifold <span class="keyword">import</span> TSNE</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成示例数据</span></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line">X = np.random.randn(<span class="number">100</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化 t-SNE 模型</span></span><br><span class="line">tsne = TSNE(n_components=<span class="number">2</span>)  <span class="comment"># 设置降维后的维度为2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 对数据进行降维</span></span><br><span class="line">X_tsne = tsne.fit_transform(X)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制降维后的数据</span></span><br><span class="line">plt.scatter(X_tsne[:, <span class="number">0</span>], X_tsne[:, <span class="number">1</span>], alpha=<span class="number">0.7</span>)</span><br><span class="line">plt.xlabel(<span class="string">'t-SNE1'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'t-SNE2'</span>)</span><br><span class="line">plt.title(<span class="string">'t-SNE降维示例'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h2 id="10-4-自编码器（Autoencoder）"><a href="#10-4-自编码器（Autoencoder）" class="headerlink" title="10.4 自编码器（Autoencoder）"></a>10.4 自编码器（Autoencoder）</h2><h3 id="10-4-1-简介"><a href="#10-4-1-简介" class="headerlink" title="10.4.1 简介"></a>10.4.1 简介</h3><p>自编码器（Autoencoder）是一种神经网络模型，通过编码器将输入数据映射到低维表示，并通过解码器将低维表示映射回原始输入数据，从而实现降维。自编码器在无监督学习中具有很大的灵活性，可以用于多种降维任务。</p>
<h3 id="10-4-2-算法"><a href="#10-4-2-算法" class="headerlink" title="10.4.2 算法"></a>10.4.2 算法</h3><ol>
<li>编码器：将输入数据压缩成潜在空间表征。</li>
<li>解码器：将潜在空间表征还原为原始数据。</li>
<li>损失函数：衡量重构输出与原始输入之间的误差。</li>
</ol>
<h3 id="10-4-3-公式"><a href="#10-4-3-公式" class="headerlink" title="10.4.3 公式"></a>10.4.3 公式</h3><ol>
<li>编码器：$h=f(x)$</li>
<li>解码器：$r=g(h)$</li>
<li>损失函数：$L(x,r)=||x-r||^2$</li>
</ol>
<p>10.4.4 代码</p>
<p>假设有一个二维数据集，其中每个样本有两个特征：$x_1$和$x_2$。想要使用自编码器将这个数据集降到一维并进行可视化。下面是Python代码实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Input, Dense</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Model</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成二维数据集</span></span><br><span class="line">X = np.random.rand(<span class="number">1000</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义自编码器模型</span></span><br><span class="line">input_layer = Input(shape=(<span class="number">2</span>,))</span><br><span class="line">encoded = Dense(<span class="number">1</span>, activation=<span class="string">'relu'</span>)(input_layer)</span><br><span class="line">decoded = Dense(<span class="number">2</span>, activation=<span class="string">'sigmoid'</span>)(encoded)</span><br><span class="line">autoencoder = Model(input_layer, decoded)</span><br><span class="line">autoencoder.compile(optimizer=<span class="string">'adam'</span>, loss=<span class="string">'mse'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练自编码器模型</span></span><br><span class="line">autoencoder.fit(X, X, epochs=<span class="number">50</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化结果</span></span><br><span class="line">encoder = Model(input_layer, encoded)</span><br><span class="line">X_encoded = encoder.predict(X)</span><br><span class="line">plt.scatter(X_encoded[:, <span class="number">0</span>], np.zeros_like(X_encoded[:, <span class="number">0</span>]))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h3 id="10-5-总结"><a href="#10-5-总结" class="headerlink" title="10.5 总结"></a>10.5 总结</h3><p>PCA、ICA、t-SNE和Autoencoder都是降维算法：</p>
<ul>
<li>PCA是一种线性降维算法，它通过正交变换将一组可能存在相关性的变量转换为一组线性不相关的变量，即把多指标转化为少数几个综合指标，转换后的这组变量就叫做主成分，其中每个主成分都能够反映原始变量的大部分信息，且所含信息互不重叠。</li>
<li>ICA是一种非线性降维算法，它是基于独立性原理的一种盲源分离算法。</li>
<li>t-SNE是一种非线性降维算法，它是基于概率分布的一种方法，可以用于高维数据的可视化。</li>
<li>Autoencoder是一种神经网络模型，它可以将高维数据压缩到低维空间中，并且可以通过解码器将低维数据还原到高维空间中</li>
</ul>

    </div>

  </article>
  <div class="toc-container">
    
  <div id="toc" class="toc-article">
    <strong class="toc-title">目录</strong>
    <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#第10章：无监督学习之降维"><span class="toc-text">第10章：无监督学习之降维</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#10-1-主成分分析（PCA）"><span class="toc-text">10.1 主成分分析（PCA）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#10-1-1-简介"><span class="toc-text">10.1.1 简介</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-1-2-算法"><span class="toc-text">10.1.2 算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-1-3-公式"><span class="toc-text">10.1.3 公式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-1-4-代码"><span class="toc-text">10.1.4 代码</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10-2-独立成分分析（ICA）"><span class="toc-text">10.2 独立成分分析（ICA）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#10-2-1-简介"><span class="toc-text">10.2.1 简介</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-2-2-算法"><span class="toc-text">10.2.2 算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-2-3-公式"><span class="toc-text">10.2.3 公式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-2-4-代码"><span class="toc-text">10.2.4 代码</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10-3-t-分布随机近邻嵌入（t-SNE）"><span class="toc-text">10.3 t-分布随机近邻嵌入（t-SNE）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#10-3-1-简介"><span class="toc-text">10.3.1 简介</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-3-2-算法"><span class="toc-text">10.3.2 算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-3-3-公式"><span class="toc-text">10.3.3 公式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-3-4-代码"><span class="toc-text">10.3.4 代码</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10-4-自编码器（Autoencoder）"><span class="toc-text">10.4 自编码器（Autoencoder）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#10-4-1-简介"><span class="toc-text">10.4.1 简介</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-4-2-算法"><span class="toc-text">10.4.2 算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-4-3-公式"><span class="toc-text">10.4.3 公式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-5-总结"><span class="toc-text">10.5 总结</span></a></li></ol></li></ol></li></ol>
  </div>


  </div>
</div>
<div class="copyright">
    <span>本作品采用</span>
    <a href="https://creativecommons.org/licenses/by-nc-nd/4.0/deed.zh" target="_blank" rel="noopener">署名-非商业性使用-禁止演绎 4.0 国际许可协议</a>
    <span>进行许可。转载时请注明原文链接。</span>
</div>


  
    <div class="post-nav">
      <div class="post-nav-item post-nav-next">
        
          <span>〈 </span>
          <a href="/post/ai-0-ml-9/" rel="next" title="机器学习系列（九）无监督学习之聚类">
          机器学习系列（九）无监督学习之聚类
          </a>
        
      </div>
  
      <div class="post-nav-item post-nav-prev">
          
          <a href="/post/ai-0-ml-11/" rel="prev" title="机器学习系列（十一）人工神经网络基础">
            机器学习系列（十一）人工神经网络基础
          </a>
          <span>〉</span>
        
      </div>
    </div>
  



</div>
</div>

<footer class="footer text-center">
    <div id="bottom-inner">
        <a class="bottom-item" href="" target="_blank">QH的博客</a> |
        <a class="bottom-item">Powered by hexo</a> |
        <a class="bottom-item">Copyright © 2023</a>  |
        <a>博客已运行<a id="days">0</a>天</a> 
        <script> 
        var s1 = '2020-04-15';//设置为你的建站时间 
        s1 = new Date(s1.replace(/-/g, "/")); 
        s2 = new Date(); 
        var days = s2.getTime() - s1.getTime(); 
        var number_of_days = parseInt(days / (1000 * 60 * 60 * 24)); 
        document.getElementById('days').innerHTML = number_of_days; 
        </script> |
        <a class="bottom-item" href="/atom.xml">RSS</a>
        <script>
			var _hmt = _hmt || [];
			(function() {
			  var hm = document.createElement("script");
			  hm.src = "https://hm.baidu.com/hm.js?7241c66da4c19bc02e939e6776919ff8";
			  var s = document.getElementsByTagName("script")[0]; 
			  s.parentNode.insertBefore(hm, s);
			})();
		</script>
    </div>
</footer>



<script>
  (function(window, document, undefined) {

    var timer = null;

    function returnTop() {
      cancelAnimationFrame(timer);
      timer = requestAnimationFrame(function fn() {
        var oTop = document.body.scrollTop || document.documentElement.scrollTop;
        if (oTop > 0) {
          document.body.scrollTop = document.documentElement.scrollTop = oTop - 50;
          timer = requestAnimationFrame(fn);
        } else {
          cancelAnimationFrame(timer);
        }
      });
    }

    var hearts = [];
    window.requestAnimationFrame = (function() {
      return window.requestAnimationFrame ||
        window.webkitRequestAnimationFrame ||
        window.mozRequestAnimationFrame ||
        window.oRequestAnimationFrame ||
        window.msRequestAnimationFrame ||
        function(callback) {
          setTimeout(callback, 1000 / 60);
        }
    })();
    init();

    function init() {
      css(".heart{z-index:9999;width: 10px;height: 10px;position: fixed;background: #f00;transform: rotate(45deg);-webkit-transform: rotate(45deg);-moz-transform: rotate(45deg);}.heart:after,.heart:before{content: '';width: inherit;height: inherit;background: inherit;border-radius: 50%;-webkit-border-radius: 50%;-moz-border-radius: 50%;position: absolute;}.heart:after{top: -5px;}.heart:before{left: -5px;}");
      attachEvent();
      gameloop();
      addMenuEvent();
    }

    function gameloop() {
      for (var i = 0; i < hearts.length; i++) {
        if (hearts[i].alpha <= 0) {
          document.body.removeChild(hearts[i].el);
          hearts.splice(i, 1);
          continue;
        }
        hearts[i].y--;
        hearts[i].scale += 0.004;
        hearts[i].alpha -= 0.013;
        hearts[i].el.style.cssText = "left:" + hearts[i].x + "px;top:" + hearts[i].y + "px;opacity:" + hearts[i].alpha + ";transform:scale(" + hearts[i].scale + "," + hearts[i].scale + ") rotate(45deg);background:" + hearts[i].color;
      }
      requestAnimationFrame(gameloop);
    }

    /**
     * 给logo设置点击事件
     * 
     * - 回到顶部
     * - 出现爱心
     */
    function attachEvent() {
      var old = typeof window.onclick === "function" && window.onclick;
      var logo = document.getElementById("logo");
      if (logo) {
        logo.onclick = function(event) {
          returnTop();
          old && old();
          createHeart(event);
        }
      }
      
    }

    function createHeart(event) {
      var d = document.createElement("div");
      d.className = "heart";
      hearts.push({
        el: d,
        x: event.clientX - 5,
        y: event.clientY - 5,
        scale: 1,
        alpha: 1,
        color: randomColor()
      });
      document.body.appendChild(d);
    }

    function css(css) {
      var style = document.createElement("style");
      style.type = "text/css";
      try {
        style.appendChild(document.createTextNode(css));
      } catch (ex) {
        style.styleSheet.cssText = css;
      }
      document.getElementsByTagName('head')[0].appendChild(style);
    }

    function randomColor() {
      // return "rgb(" + (~~(Math.random() * 255)) + "," + (~~(Math.random() * 255)) + "," + (~~(Math.random() * 255)) + ")";
      return "#F44336";
    }

    function addMenuEvent() {
      var menu = document.getElementById('menu-main-post');
      if (menu) {
        var toc = document.getElementById('toc');
        if (toc) {
          menu.onclick = function() {
            if (toc) {
              if (toc.style.display == 'block') {
                toc.style.display = 'none';
              } else {
                toc.style.display = 'block';
              }
            }
          };
        } else {
          menu.style.display = 'none';
        }
      }
    }

  })(window, document);
</script>






<script>
  (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      if (curProtocol === 'https') {
          bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
      }
      else {
          bp.src = 'http://push.zhanzhang.baidu.com/push.js';
      }
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
  })();
</script>


<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
</body>
</html>