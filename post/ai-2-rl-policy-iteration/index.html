<!DOCTYPE html>
<html lang="zh-CN">


<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1.0, user-scalable=no">
  <meta name="theme-color" content="#202020"/>
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  
    <meta name="keywords" content="机器学习,强化学习," />
  

  
    <meta name="description" content="QH的博客" />
  
  
  
  <link rel="icon" type="image/x-icon" href="/images/favicon.ico">
  
  <title>强化学习笔记：策略迭代与价值迭代及其代码实现 - QH的博客</title>
  
    <!-- stylesheets list from config.yml -->
    
      <link rel="stylesheet" href="/css/pure-min.css">
    
      <link rel="stylesheet" href="/css/xoxo.css">
    
  
<meta name="generator" content="Hexo 4.2.1"><link rel="alternate" href="/atom.xml" title="QH的博客" type="application/atom+xml">
</head>


<body>
<div class="nav-container">
<nav class="home-menu pure-menu pure-menu-horizontal">
  <a class="pure-menu-heading" href="/">
    
      <img class="avatar" src="https://smiler666.github.io/images/favicon.ico">
    
    <span class="title" style="text-transform:none">QH的博客</span>
  </a>

  <ul class="pure-menu-list clearfix">
      
          
            
              <li class="pure-menu-item"><a href="/" class="pure-menu-link">首页</a></li>
            
          
      
          
            
              <li class="pure-menu-item"><a href="/archives" class="pure-menu-link">文章</a></li>
            
          
      
          
            
              <li class="pure-menu-item"><a href="/post/share-1/" class="pure-menu-link">工具</a></li>
            
          
      
          
            
              <li class="pure-menu-item"><a href="/about" class="pure-menu-link">关于</a></li>
            
          
      
          
            
              <li class="pure-menu-item"><a href="/search" class="pure-menu-link">搜索</a></li>
            
          
      
  </ul>
   
</nav>

</div>

<div class="container" id="content-outer">
<div class="inner" id="content-inner">
<div class="post-container">
  <article class="post" id="post">
    <header class="post-header text-center">
      <h1 class="title">
        强化学习笔记：策略迭代与价值迭代及其代码实现
      </h1>
      <span>
        
        <time class="time" datetime="2020-04-28T04:42:30.000Z">
        2020-04-28
      </time>
        
      </span>
      <span class="slash">/</span>
      <span class="post-meta">
      <span class="post-tags">
        <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" rel="tag">强化学习</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li></ul>
      </span>
    </span>
      <span class="slash">/</span>
      <span class="read">阅读耗时 16 分钟</span>
    </header>

    <div class="post-content">
      <a id="more"></a>
<h1 id="策略迭代"><a href="#策略迭代" class="headerlink" title="策略迭代"></a>策略迭代</h1><p>策略迭代包括<strong>策略评估(Policy Evaluation)</strong>和<strong>策略改进(Policy Improvement)</strong>。</p>
<p>基本过程是从一个初始化的策略出发，先进行策略评估，然后策略改进，评估改进的策略，再进一步改进策略，经过不断迭代更新，直达策略收敛。</p>
<p><img src="https://cdn.jsdelivr.net/gh/smiler666/smiler666.github.io/post/ai-2-rl-policy-iteration/1.png" alt="策略评估和策略改进"></p>
<p><strong>算法的伪代码如下：</strong></p>
<p><img src="https://cdn.jsdelivr.net/gh/smiler666/smiler666.github.io/post/ai-2-rl-policy-iteration/2.png" alt="策略迭代算法伪代码"></p>
<p><strong>算法过程：</strong></p>
<p><strong>1. Initialization</strong></p>
<p>初始化所有状态的$v(s)$以及$π(s)$（初始化为随机策略） </p>
<p><strong>2. Policy evaluation</strong> </p>
<p>用当前的$v(s)$对当前策略$π(s)$进行评估，计算出每一个状态的$v(s)$，直到$v(s)$收敛，才算训练好了这个状态价值函数$V(s)$</p>
<p><strong>3. Policy improvement</strong> </p>
<p>上一步已经得到了当前策略的评估函数$V(s)$,那么就可以利用这个评估函数进行策略改进啦。 在每个状态$s$时，对每个可能的动作$a$,都计算一下采取这个动作后到达的下一个状态的期望价值。看看哪个动作可以到达的状态的期望价值函数最大，就选取这个动作。以此更新了$π(s) $</p>
<p>然后再次<strong>循环上述２、３步骤，直到$V(s)$与$π(s)$都收敛</strong>。</p>
<h2 id="1-策略评估"><a href="#1-策略评估" class="headerlink" title="1.策略评估"></a>1.策略评估</h2><p>策略评估就是计算任意策略的状态值函数$V_\pi$，也就是对当前策略下计算出每个状态下的状态值，这就是策略预估，我们也把它称为<strong>预测</strong>(predict)问题。</p>
<script type="math/tex; mode=display">
v_{k+1}(s_t)=\sum_{a∈A}\pi(a|s)[R(s,a)+\gamma \sum_{s∈S}p(s'|s,a)v_t(s')]</script><p>下面介绍一个策略评估的例子：</p>
<p>这是一个经典的Grid World的例子。这里有一个$4*4$的的格子世界。只有左上和右下的格子是终止格子。该位置的价值固定为0，个体如果到达了该2个格子，则停止移动，此后每轮奖励都是0。个体在16宫格其他格的每次移动，得到的即时奖励$R$都是-1。注意个体每次只能移动一个格子，且只能上下左右4种移动选择，不能斜着走, 如果在边界格往外走，则会直接移动回到之前的边界格。衰减因子我们定义为$γ=1$。由于这里每次移动，下一格都是固定的，因此所有可行的的状态转化概率$P=1$。这里给定的策略是随机策略，即每个格子里有25%的概率向周围的4个格子移动。</p>
<p><img src="https://cdn.jsdelivr.net/gh/smiler666/smiler666.github.io/post/ai-2-rl-policy-iteration/3.jpg" alt></p>
<p>首先我们初始化所有格子的状态价值为0，如上图k=0的时候。现在我们开始策略迭代了。由于终止格子的价值固定为0，我们可以不将其加入迭代过程。在$k=1$的时候，我们利用上面的贝尔曼方程先计算第二行第一个格子的价值：</p>
<script type="math/tex; mode=display">v_1^{21}=\frac{1}{4}[(−1+0)+(−1+0)+(−1+0)+(−1+0)]=−1</script><p>第二行第二个格子的价值是：</p>
<script type="math/tex; mode=display">v_1^{22}=\frac{1}{4}[(−1+0)+(−1+0)+(−1+0)+(−1+0)]=−1</script><p>其余的以此类推，第一轮的状态价值迭代的结果如上图$k=1$的时候。现在我们第一轮迭代完了。开始动态规划迭代第二轮了。还是看第二行第一个格子的价值：</p>
<script type="math/tex; mode=display">v_2^{21}=\frac{1}{4}[(−1+0)+(−1-1)+(−1-1)+(−1-1)]=−1.75</script><p>第二行第二个格子的价值是：</p>
<script type="math/tex; mode=display">v_2^{22}=\frac{1}{4}[(−1-1)+(−1-1)+(−1-1)+(−1-1)]=−2</script><p>最终得到的结果是上图k=2的时候。第三轮的迭代如下：</p>
<script type="math/tex; mode=display">v_3^{21}=\frac{1}{4}[(−1-1.7)+(−1-2)+(−1-2)+(−1+0)]=−2.425</script><script type="math/tex; mode=display">v_3^{22}=\frac{1}{4}[(−1-1.7)+(−1-1.7)+(−1-2)+(−1-2)]=−2.85</script><p>最终得到的结果是上图$k=3$的时候。就这样一直迭代下去，直到每个格子的策略价值改变很小为止。这时我们就得到了所有格子的基于随机策略的状态价值。</p>
<p>可以看到，动态规划的策略评估计算过程并不复杂，但是如果我们的问题是一个非常复杂的模型的话，这个计算量还是非常大的。</p>
<h2 id="2-策略改进"><a href="#2-策略改进" class="headerlink" title="2.策略改进"></a>2.策略改进</h2><p>上面我们讲了使用策略评估求解了<strong>预测</strong>问题，现在还有<strong>控制</strong>问题没有解决。我们计算策略的状态价值函数的原因是为了帮助找到更好的策略。因为我们上一步得到了上一个策略下的每个状态下的状态值，所以接下来计算状态-行动价值函数，对策略进行改进，计算出新的策略。</p>
<p>计算方式如下：</p>
<p>遍历每一个状态，对每个可能的动作$a$，都计算一下采取这个动作后到达的下一个状态的期望价值。用贪婪法看看哪个动作可以到达的状态的期望价值函数最大，就选取这个动作，以此更新了$π(s)$公式如下：</p>
<script type="math/tex; mode=display">q^{\pi_i}(s,a)=[R(s,a)+\gamma \sum_{s'∈S}p(s'|st,a)v^{\pi_i}(s')]</script><script type="math/tex; mode=display">\pi_{i+1}(s)={\underset {a}{\operatorname {arg\max} }}q^{\pi_i}(s,a)</script><h1 id="价值迭代"><a href="#价值迭代" class="headerlink" title="价值迭代"></a>价值迭代</h1><p>价值迭代就比较简单了，和策略迭代差不多，对每一个当前状态$s$ ,对每个可能的动作$a$ 都计算一下采取这个动作后到达的下一个状态的期望价值。看看哪个动作可以到达的状态的期望价值函数最大，就将这个最大的期望价值函数作为当前状态的价值函数 $V(s)$循环执行这个步骤，直到价值函数收敛。<br>期望价值计算公式：</p>
<script type="math/tex; mode=display">q_{k+1}(s,a)=R(s,a)+\gamma \sum_{s∈S} P(s'|s,a)v_k(s')</script><script type="math/tex; mode=display">V_{k+1}(s)={\underset {a}{\operatorname {max} }}q_{k+1}(s,a)</script><p><strong>算法的伪代码如下：</strong></p>
<p><img src="https://cdn.jsdelivr.net/gh/smiler666/smiler666.github.io/post/rl-policy-iteration/5.png" alt></p>
<p><strong>价值迭代算法过程：</strong></p>
<p><strong>１.Initialization</strong><br>初始化所有状态的$v(s)$<br><strong>２.Finding optimal value function（找到最优的价值函数）</strong><br>注意伪代码里的$max$，对每一个当前状态$s$,对每个可能的动作$a$,都计算一下采取这个动作后到达的下一个状态的期望价值。看看哪个动作可以到达的状态的期望价值函数最大，就将这个最大的期望价值函数作为当前状态的价值函数$v(s)$循环执行这个步骤，直到价值函数收敛,就可以得到最优的价值函数了<br><strong>3.Policy extraction</strong><br>利用上面步骤得到的最优价值函数，就可得到每个状态应该采取的最优动作,这个是我们的deterministic policy。</p>
<p>还是以下图为例，我们如果用贪婪法调整动作策略，那么当k=3的时候，我们就已经得到了最优的动作策略。而不用一直迭代到状态价值收敛才去调整策略。那么此时我们的策略迭代优化为价值迭代。</p>
<p><img src="https://cdn.jsdelivr.net/gh/smiler666/smiler666.github.io/post/ai-2-rl-policy-iteration/3.jpg" alt></p>
<p>如上面的图右边。比如当$k=2$时，第二行第一个格子周围的价值分别是0,-2,-2，此时我们用贪婪法，则我们调整行动策略为向状态价值为0的方向移动，而不是随机移动。也就是图中箭头向上。而此时第二行第二个格子周围的价值分别是-1.7,-1.7,-2, -2。那么我们整行动策略为向状态价值为-1.7的方向移动，也就是图中的向左向上。</p>
<p>和策略迭代相比，我们没有等到状态价值收敛才调整策略，而是随着状态价值的迭代及时调整策略, 这样可以大大减少迭代次数。此时我们的状态价值的更新方法也和策略迭代不同。可见由于策略调整，我们现在价值每次更新倾向于贪婪法选择的最优策略对应的后续状态价值，这样收敛更快。</p>
<h1 id="两者区别"><a href="#两者区别" class="headerlink" title="两者区别"></a>两者区别</h1><p>一开始没有理解策略迭代和价值迭代的区别，导致后面的学习稀里糊涂。</p>
<ul>
<li>对于价值迭代，包括<strong>ﬁnding optimal value function</strong> + <strong>one policy extraction</strong>。状态价值每更新一次，策略就更新一次。二者不存在重复，因为一旦价值函数收敛后，那么它的策略也是最优的（即收敛）。</li>
<li>对于策略迭代，包括策略评估和策略改进，我们只有在状态价值更新到<strong>收敛</strong>后，才会更新策略（策略改进），二者重读迭代，直到策略收敛。</li>
<li>策略迭代的收敛速度更快一些，在状态空间较小时，最好选用策略迭代方法。当状态空间较大时，值迭代的计算量更小一些。</li>
</ul>
<h1 id="代码-例子"><a href="#代码-例子" class="headerlink" title="代码(例子)"></a>代码(例子)</h1><p>下面给出一个具体的例子</p>
<p><img src="https://cdn.jsdelivr.net/gh/smiler666/smiler666.github.io/post/ai-2-rl-policy-iteration/6.jpg" alt="动态规划中价值迭代和策略迭代的例子"></p>
<p>MDP的五元组分别为：</p>
<ul>
<li>状态：1和2</li>
<li>动作：图中红色线表示动作a，蓝色线表示动作b</li>
<li>回报：R(1) = 0，R(2) = 1</li>
<li>转移概率：如图所示</li>
<li>折扣因子：0.9</li>
</ul>
<p>代码放在我的GitHub上了，需要的请前往查看：</p>
<p>价值迭代：<a href="https://github.com/smiler666/Reinforcement-Learning-Notes/blob/master/value%20iteration.py" target="_blank" rel="noopener">前往查看代码</a></p>
<p>策略迭代：<a href="https://github.com/smiler666/Reinforcement-Learning-Notes/blob/master/policy%20iteration.py" target="_blank" rel="noopener">前往查看代码</a></p>
<hr>
<blockquote>
<p>若理解有误，欢迎指正。</p>
</blockquote>
<p>参考：</p>
<ul>
<li><a href="https://www.cnblogs.com/pinard/p/9463815.html" target="_blank" rel="noopener">https://www.cnblogs.com/pinard/p/9463815.html</a></li>
<li><a href="https://blog.csdn.net/panglinzhuo/article/details/77752574" target="_blank" rel="noopener">https://blog.csdn.net/panglinzhuo/article/details/77752574</a></li>
<li>周博磊《强化学习纲要》课件</li>
</ul>

    </div>

  </article>
  <div class="toc-container">
    
  <div id="toc" class="toc-article">
    <strong class="toc-title">目录</strong>
    <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#策略迭代"><span class="toc-text">策略迭代</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-策略评估"><span class="toc-text">1.策略评估</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-策略改进"><span class="toc-text">2.策略改进</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#价值迭代"><span class="toc-text">价值迭代</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#两者区别"><span class="toc-text">两者区别</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#代码-例子"><span class="toc-text">代码(例子)</span></a></li></ol>
  </div>


  </div>
</div>
<div class="copyright">
    <span>本作品采用</span>
    <a href="https://creativecommons.org/licenses/by-nc-nd/4.0/deed.zh" target="_blank" rel="noopener">署名-非商业性使用-禁止演绎 4.0 国际许可协议</a>
    <span>进行许可。转载时请注明原文链接。</span>
</div>


  
    <div class="post-nav">
      <div class="post-nav-item post-nav-next">
        
          <span>〈 </span>
          <a href="/post/System-Simulation-2/" rel="next" title="系统仿真笔记（二）建模方法概述">
          系统仿真笔记（二）建模方法概述
          </a>
        
      </div>
  
      <div class="post-nav-item post-nav-prev">
          
          <a href="/post/System-Simulation-3/" rel="prev" title="系统仿真笔记（三）离散事件系统仿真基础">
            系统仿真笔记（三）离散事件系统仿真基础
          </a>
          <span>〉</span>
        
      </div>
    </div>
  



</div>
</div>

<footer class="footer text-center">
    <div id="bottom-inner">
        <a class="bottom-item" href="" target="_blank">QH的博客</a> |
        <a class="bottom-item">Powered by hexo</a> |
        <a class="bottom-item">Copyright © 2023</a>  |
        <a>博客已运行<a id="days">0</a>天</a> 
        <script> 
        var s1 = '2020-04-15';//设置为你的建站时间 
        s1 = new Date(s1.replace(/-/g, "/")); 
        s2 = new Date(); 
        var days = s2.getTime() - s1.getTime(); 
        var number_of_days = parseInt(days / (1000 * 60 * 60 * 24)); 
        document.getElementById('days').innerHTML = number_of_days; 
        </script> |
        <a class="bottom-item" href="/atom.xml">RSS</a>
        <script>
			var _hmt = _hmt || [];
			(function() {
			  var hm = document.createElement("script");
			  hm.src = "https://hm.baidu.com/hm.js?7241c66da4c19bc02e939e6776919ff8";
			  var s = document.getElementsByTagName("script")[0]; 
			  s.parentNode.insertBefore(hm, s);
			})();
		</script>
    </div>
</footer>



<script>
  (function(window, document, undefined) {

    var timer = null;

    function returnTop() {
      cancelAnimationFrame(timer);
      timer = requestAnimationFrame(function fn() {
        var oTop = document.body.scrollTop || document.documentElement.scrollTop;
        if (oTop > 0) {
          document.body.scrollTop = document.documentElement.scrollTop = oTop - 50;
          timer = requestAnimationFrame(fn);
        } else {
          cancelAnimationFrame(timer);
        }
      });
    }

    var hearts = [];
    window.requestAnimationFrame = (function() {
      return window.requestAnimationFrame ||
        window.webkitRequestAnimationFrame ||
        window.mozRequestAnimationFrame ||
        window.oRequestAnimationFrame ||
        window.msRequestAnimationFrame ||
        function(callback) {
          setTimeout(callback, 1000 / 60);
        }
    })();
    init();

    function init() {
      css(".heart{z-index:9999;width: 10px;height: 10px;position: fixed;background: #f00;transform: rotate(45deg);-webkit-transform: rotate(45deg);-moz-transform: rotate(45deg);}.heart:after,.heart:before{content: '';width: inherit;height: inherit;background: inherit;border-radius: 50%;-webkit-border-radius: 50%;-moz-border-radius: 50%;position: absolute;}.heart:after{top: -5px;}.heart:before{left: -5px;}");
      attachEvent();
      gameloop();
      addMenuEvent();
    }

    function gameloop() {
      for (var i = 0; i < hearts.length; i++) {
        if (hearts[i].alpha <= 0) {
          document.body.removeChild(hearts[i].el);
          hearts.splice(i, 1);
          continue;
        }
        hearts[i].y--;
        hearts[i].scale += 0.004;
        hearts[i].alpha -= 0.013;
        hearts[i].el.style.cssText = "left:" + hearts[i].x + "px;top:" + hearts[i].y + "px;opacity:" + hearts[i].alpha + ";transform:scale(" + hearts[i].scale + "," + hearts[i].scale + ") rotate(45deg);background:" + hearts[i].color;
      }
      requestAnimationFrame(gameloop);
    }

    /**
     * 给logo设置点击事件
     * 
     * - 回到顶部
     * - 出现爱心
     */
    function attachEvent() {
      var old = typeof window.onclick === "function" && window.onclick;
      var logo = document.getElementById("logo");
      if (logo) {
        logo.onclick = function(event) {
          returnTop();
          old && old();
          createHeart(event);
        }
      }
      
    }

    function createHeart(event) {
      var d = document.createElement("div");
      d.className = "heart";
      hearts.push({
        el: d,
        x: event.clientX - 5,
        y: event.clientY - 5,
        scale: 1,
        alpha: 1,
        color: randomColor()
      });
      document.body.appendChild(d);
    }

    function css(css) {
      var style = document.createElement("style");
      style.type = "text/css";
      try {
        style.appendChild(document.createTextNode(css));
      } catch (ex) {
        style.styleSheet.cssText = css;
      }
      document.getElementsByTagName('head')[0].appendChild(style);
    }

    function randomColor() {
      // return "rgb(" + (~~(Math.random() * 255)) + "," + (~~(Math.random() * 255)) + "," + (~~(Math.random() * 255)) + ")";
      return "#F44336";
    }

    function addMenuEvent() {
      var menu = document.getElementById('menu-main-post');
      if (menu) {
        var toc = document.getElementById('toc');
        if (toc) {
          menu.onclick = function() {
            if (toc) {
              if (toc.style.display == 'block') {
                toc.style.display = 'none';
              } else {
                toc.style.display = 'block';
              }
            }
          };
        } else {
          menu.style.display = 'none';
        }
      }
    }

  })(window, document);
</script>






<script>
  (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      if (curProtocol === 'https') {
          bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
      }
      else {
          bp.src = 'http://push.zhanzhang.baidu.com/push.js';
      }
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
  })();
</script>


</body>
</html>