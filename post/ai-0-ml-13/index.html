<!DOCTYPE html>
<html lang="zh-CN">


<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1.0, user-scalable=no">
  <meta name="theme-color" content="#202020"/>
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  
    <meta name="keywords" content="机器学习,深度学习,强化学习," />
  

  
    <meta name="description" content="QH的博客" />
  
  
  
  <link rel="icon" type="image/x-icon" href="/images/favicon.ico">
  
  <title>机器学习系列（十三）循环神经网路 - QH的博客</title>
  
    <!-- stylesheets list from config.yml -->
    
      <link rel="stylesheet" href="/css/pure-min.css">
    
      <link rel="stylesheet" href="/css/xoxo.css">
    
  
<meta name="generator" content="Hexo 4.2.1"><link rel="alternate" href="/atom.xml" title="QH的博客" type="application/atom+xml">
</head>


<body>
<div class="nav-container">
<nav class="home-menu pure-menu pure-menu-horizontal">
  <a class="pure-menu-heading" href="/">
    
      <img class="avatar" src="https://smiler666.github.io/images/favicon.ico">
    
    <span class="title" style="text-transform:none">QH的博客</span>
  </a>

  <ul class="pure-menu-list clearfix">
      
          
            
              <li class="pure-menu-item"><a href="/" class="pure-menu-link">首页</a></li>
            
          
      
          
            
              <li class="pure-menu-item"><a href="/archives" class="pure-menu-link">文章</a></li>
            
          
      
          
            
              <li class="pure-menu-item"><a href="/about" class="pure-menu-link">关于</a></li>
            
          
      
          
            
              <li class="pure-menu-item"><a href="/search" class="pure-menu-link">搜索</a></li>
            
          
      
  </ul>
   
</nav>

</div>

<div class="container" id="content-outer">
<div class="inner" id="content-inner">
<div class="post-container">
  <article class="post" id="post">
    <header class="post-header text-center">
      <h1 class="title">
        机器学习系列（十三）循环神经网路
      </h1>
      <span>
        
        <time class="time" datetime="2021-04-03T04:20:00.000Z">
        2021-04-03
      </time>
        
      </span>
      <span class="slash">/</span>
      <span class="post-meta">
      <span class="post-tags">
        <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" rel="tag">强化学习</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag">深度学习</a></li></ul>
      </span>
    </span>
      <span class="slash">/</span>
      <span class="read">阅读耗时 16 分钟</span>
    </header>

    <div class="post-content">
      <a id="more"></a>
<h1 id="第13章：循环神经网路"><a href="#第13章：循环神经网路" class="headerlink" title="第13章：循环神经网路"></a>第13章：循环神经网路</h1><p>循环神经网络（Recurrent Neural Network，RNN）是一类具有短期记忆能力的神经网络，适合用于处理视频、语音、文本等与时序相关的问题。在循环神经网络中，神经元不但可以接收其他神经元的信息，还可以接收自身的信息，形成具有环路的网络结构。循环神经网络的参数学习可以通过随时间反向传播算法来学习，即按照时间的逆序把误差一步步往前传递。而当输入序列比较长时，会产生梯度爆炸或梯度消失问题，这也叫做长期依赖问题。为了解决这个问题，门控机制被引入来改进循环神经网络，也就是长短期记忆网络（LSTM）和门控循环单元（GRU）。</p>
<h2 id="13-1-基本结构"><a href="#13-1-基本结构" class="headerlink" title="13.1 基本结构"></a>13.1 基本结构</h2><p>循环神经网络可以描述动态时间行为，因为和前馈神经网络（feedforward neural network）接受较特定结构的输入不同，RNN将状态在自身网络中循环传递，因此可以接受更广泛的时间序列结构输入。循环神经网络由输入层、隐藏层和输出层组成，其中隐藏层是循环层，每个隐藏单元都有一个自连接，并且所有隐藏单元共享相同的权重矩阵。</p>
<p>假设输入序列为 ${\vec x}=(x_1,x_2,…,x_T)$ ，输出序列为 ${\vec y}=(y_1,y_2,…,y_T)$ ，则循环神经网络的运算过程如下：</p>
<ul>
<li>对于每个时刻 $t=1,2,…,T$ ，首先计算隐藏状态 ${\vec h}_t$ ，它由当前时刻的输入 $x_t$ 和上一时刻的隐藏状态 ${\vec h}_{t-1}$ 共同决定：</li>
</ul>
<script type="math/tex; mode=display">
{\vec h}_t=f(U{\vec x}_t+W{\vec h}_{t-1})</script><p>其中 $U$ 和 $W$ 是输入到隐藏和隐藏到隐藏的权重矩阵， $f$ 是一个非线性激活函数，如 $\tanh$ 或 $\mathrm{ReLU}$ 。</p>
<ul>
<li>然后根据隐藏状态 ${\vec h}_t$ 计算当前时刻的输出 ${\vec y}_t$ ，它由一个线性变换和一个可选的激活函数或归一化函数组成：</li>
</ul>
<script type="math/tex; mode=display">
{\vec y}_t=g(V{\vec h}_t)</script><p>其中 $V$ 是隐藏到输出的权重矩阵， $g$ 是一个可选的函数，如 $\mathrm{softmax}$ 或 $\mathrm{sigmoid}$ 。</p>
<ul>
<li>最后，根据输出 ${\vec y}_t$ 和真实标签 ${\vec d}_t$ 计算损失函数 $L_t$ ，它可以是交叉熵、均方误差或其他形式：</li>
</ul>
<script type="math/tex; mode=display">
L_t=L({\vec y}_t,{\vec d}_t)</script><p>循环神经网络的总损失函数是所有时刻的损失函数之和：</p>
<script type="math/tex; mode=display">
L=\sum_{t=1}^T L_t</script><p>循环神经网络的训练目标是通过梯度下降或其他优化算法，找到一组最优的权重矩阵 $U,W,V$ ，使得总损失函数 $L$ 最小化。</p>
<h2 id="13-2-梯度计算"><a href="#13-2-梯度计算" class="headerlink" title="13.2 梯度计算"></a>13.2 梯度计算</h2><p>循环神经网络的梯度计算需要使用随时间反向传播（Backpropagation Through Time，BPTT）算法，它是一种基于动态规划的方法，可以高效地计算循环神经网络中每个权重矩阵的梯度。BPTT算法的基本思想是，首先按照时间的正序计算每个时刻的隐藏状态和输出，然后按照时间的逆序计算每个时刻的误差和梯度，并将相同权重矩阵的梯度累加起来。</p>
<p>具体来说，BPTT算法的步骤如下：</p>
<ul>
<li>初始化权重矩阵 $U,W,V$ 的梯度为零矩阵： $\Delta U=\Delta W=\Delta V=0$ 。</li>
<li>对于每个时刻 $t=1,2,…,T$ ，正向传播计算隐藏状态 ${\vec h}_t$ 和输出 ${\vec y}_t$ ：</li>
</ul>
<script type="math/tex; mode=display">
{\vec h}_t=f(U{\vec x}_t+W{\vec h}_{t-1})</script><script type="math/tex; mode=display">
{\vec y}_t=g(V{\vec h}_t)</script><ul>
<li>对于每个时刻 $t=T,T-1,…,1$ ，反向传播计算误差 $\delta_t$ 和权重矩阵的梯度 $\frac{\partial L}{\partial U},\frac{\partial L}{\partial W},\frac{\partial L}{\partial V}$ ：</li>
</ul>
<script type="math/tex; mode=display">
\delta_t=\frac{\partial L_t}{\partial {\vec y}_t}g'(V{\vec h}_t)</script><script type="math/tex; mode=display">
\frac{\partial L}{\partial V}=\frac{\partial L}{\partial V}+\delta_t{\vec h}_t^T</script><script type="math/tex; mode=display">
\Delta V=\Delta V+\frac{\partial L}{\partial V}</script><script type="math/tex; mode=display">
\frac{\partial L}{\partial {\vec h}_t}=\delta_tV^T+\frac{\partial L}{\partial {\vec h}_{t+1}}W^T</script><script type="math/tex; mode=display">
\frac{\partial L}{\partial U}=\frac{\partial L}{\partial U}+\frac{\partial L}{\partial {\vec h}_t}f'(U{\vec x}_t+W{\vec h}_{t-1}){\vec x}_t^T</script><script type="math/tex; mode=display">
\Delta U=\Delta U+\frac{\partial L}{\partial U}</script><script type="math/tex; mode=display">
\frac{\partial L}{\partial W}=\frac{\partial L}{\partial W}+\frac{\partial L}{\partial {\vec h}_t}f'(U{\vec x}_t+W{\vec h}_{t-1}){\vec h}_{t-1}^T</script><script type="math/tex; mode=display">
\Delta W=\Delta W+\frac{\partial L}{\partial W}</script><ul>
<li>根据权重矩阵的梯度 $\Delta U,\Delta W,\Delta V$ ，更新权重矩阵 $U,W,V$ ：</li>
</ul>
<script type="math/tex; mode=display">
U=U-\alpha \Delta U</script><script type="math/tex; mode=display">
W=W-\alpha \Delta W</script><script type="math/tex; mode=display">
V=V-\alpha \Delta V</script><p>其中 $\alpha$ 是学习率，控制更新的步长。</p>
<h2 id="13-3-应用实例"><a href="#13-3-应用实例" class="headerlink" title="13.3 应用实例"></a>13.3 应用实例</h2><p>循环神经网络可以应用于多种与时序相关的任务，例如语言模型、机器翻译、语音识别、文本生成等。在这里，以一个简单的文本生成任务为例，介绍如何使用Keras搭建一个循环神经网络，并用它来生成一些有趣的文本。</p>
<h3 id="13-3-1-数据准备"><a href="#13-3-1-数据准备" class="headerlink" title="13.3.1 数据准备"></a>13.3.1 数据准备</h3><p>文本生成的数据可以是任何有意义的文本，例如小说、新闻、歌词等。在这里，使用《爱丽丝梦游仙境》（Alice’s Adventures in Wonderland）这本经典的童话作为数据源，它由英国作家刘易斯·卡罗尔（Lewis Carroll）于1865年出版，讲述了一个名叫爱丽丝的小女孩跟随一只白兔进入了一个奇幻的世界，并遇到了各种奇怪的人和事的故事。可以从网上下载这本书的英文版文本文件，并用Python读取它：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入必要的库</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense, LSTM</span><br><span class="line"><span class="keyword">from</span> keras.optimizers <span class="keyword">import</span> RMSprop</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取文本文件</span></span><br><span class="line">filename = <span class="string">"alice.txt"</span></span><br><span class="line"><span class="keyword">with</span> open(filename, encoding=<span class="string">"utf-8"</span>) <span class="keyword">as</span> f:</span><br><span class="line">    text = f.read().lower()</span><br><span class="line">print(<span class="string">"Corpus length:"</span>, len(text))</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Corpus length: 144395</span><br></pre></td></tr></table></figure>
<p>可以看到，这本书的文本长度大约有14万多个字符。为了训练循环神经网络，需要将文本切分为固定长度的序列，并将每个字符转换为一个整数索引。可以用Keras提供的一个工具类来实现这个功能：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置序列长度和滑动步长</span></span><br><span class="line">maxlen = <span class="number">40</span> <span class="comment"># 每个序列包含40个字符</span></span><br><span class="line">step = <span class="number">3</span> <span class="comment"># 每隔3个字符取一个序列</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建字符到索引的映射字典</span></span><br><span class="line">chars = sorted(list(set(text))) <span class="comment"># 对文本中出现的所有字符去重并排序</span></span><br><span class="line">print(<span class="string">"Total chars:"</span>, len(chars)) <span class="comment"># 统计字符的种类数</span></span><br><span class="line">char_indices = dict((char, chars.index(char)) <span class="keyword">for</span> char <span class="keyword">in</span> chars) <span class="comment"># 生成字符到索引的映射</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 切分文本并转换为索引</span></span><br><span class="line">sentences = [] <span class="comment"># 存储切分后的序列</span></span><br><span class="line">next_chars = [] <span class="comment"># 存储每个序列之后的下一个字符</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(text) - maxlen, step): <span class="comment"># 从文本的第一个字符开始，每隔step个字符，取长度为maxlen的序列，直到文本结束</span></span><br><span class="line">    sentences.append(text[i: i + maxlen]) <span class="comment"># 取出一个序列并存入sentences列表</span></span><br><span class="line">    next_chars.append(text[i + maxlen]) <span class="comment"># 取出序列后的下一个字符并存入next_chars列表</span></span><br><span class="line">print(<span class="string">"Number of sequences:"</span>, len(sentences)) <span class="comment"># 打印切分后的序列数量</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将字符转换为one-hot编码的向量</span></span><br><span class="line">x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool) <span class="comment"># 创建一个形状为(序列数量, 序列长度, 字符种类数)的全零张量，用于存储输入数据</span></span><br><span class="line">y = np.zeros((len(sentences), len(chars)), dtype=np.bool) <span class="comment"># 创建一个形状为(序列数量, 字符种类数)的全零张量，用于存储目标数据（下一个字符）</span></span><br><span class="line"><span class="keyword">for</span> i, sentence <span class="keyword">in</span> enumerate(sentences): <span class="comment">#</span></span><br><span class="line">    <span class="keyword">for</span> t, char <span class="keyword">in</span> enumerate(sentence): <span class="comment"># 遍历每个序列中的每个字符</span></span><br><span class="line">        x[i, t, char_indices[char]] = <span class="number">1</span> <span class="comment"># 将对应位置的元素设为1，表示该字符在该位置出现</span></span><br><span class="line">    y[i, char_indices[next_chars[i]]] = <span class="number">1</span> <span class="comment"># 将对应位置的元素设为1，表示下一个字符是该字符</span></span><br></pre></td></tr></table></figure>
<h3 id="13-3-2-模型构建"><a href="#13-3-2-模型构建" class="headerlink" title="13.3.2 模型构建"></a>13.3.2 模型构建</h3><p>接下来，使用Keras搭建一个简单的循环神经网络，它由一个LSTM层和一个全连接层组成。LSTM层的作用是学习输入序列中的长期依赖关系，并输出一个固定长度的向量。全连接层的作用是根据LSTM层的输出，预测下一个字符的概率分布。使用softmax激活函数和交叉熵损失函数，以及RMSprop优化器来训练模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构建模型</span></span><br><span class="line">model = Sequential() <span class="comment"># 创建一个Sequential模型</span></span><br><span class="line">model.add(LSTM(<span class="number">128</span>, input_shape=(maxlen, len(chars)))) <span class="comment"># 添加一个LSTM层，有128个隐藏单元，输入形状为(序列长度, 字符种类数)</span></span><br><span class="line">model.add(Dense(len(chars), activation=<span class="string">"softmax"</span>)) <span class="comment"># 添加一个全连接层，有字符种类数个输出单元，激活函数为softmax</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 编译模型</span></span><br><span class="line">model.compile(loss=<span class="string">"categorical_crossentropy"</span>, optimizer=RMSprop(lr=<span class="number">0.01</span>)) <span class="comment"># 使用交叉熵损失函数和RMSprop优化器，学习率为0.01</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看模型结构</span></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Model: &quot;sequential&quot;</span><br><span class="line">_________________________________________________________________</span><br><span class="line">Layer (type)                 Output Shape              Param #   </span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">lstm (LSTM)                  (None, 128)               98816     </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense (Dense)                (None, 57)                7353      </span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">Total params: 106,169</span><br><span class="line">Trainable params: 106,169</span><br><span class="line">Non-trainable params: 0</span><br><span class="line">_________________________________________________________________</span><br></pre></td></tr></table></figure>
<p>可以看到，模型有两层，共有10万多个可训练参数。</p>
<h2 id="13-4-总结"><a href="#13-4-总结" class="headerlink" title="13.4 总结"></a>13.4 总结</h2><ul>
<li>循环神经网络是一类具有短期记忆能力的神经网络，适合用于处理视频、语音、文本等与时序相关的问题。</li>
<li>循环神经网络的基本结构由输入层、隐藏层和输出层组成，其中隐藏层是循环层，每个隐藏单元都有一个自连接，并且所有隐藏单元共享相同的权重矩阵。</li>
<li>循环神经网络的梯度计算需要使用随时间反向传播算法，它是一种基于动态规划的方法，可以高效地计算循环神经网络中每个权重矩阵的梯度。</li>
<li>循环神经网络可以应用于多种与时序相关的任务，例如语言模型、机器翻译、语音识别、文本生成等。我们以一个简单的文本生成任务为例，介绍了如何使用Keras搭建一个循环神经网络，并用它来生成一些有趣的文本。</li>
</ul>

    </div>

  </article>
  <div class="toc-container">
    
  <div id="toc" class="toc-article">
    <strong class="toc-title">目录</strong>
    <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#第13章：循环神经网路"><span class="toc-text">第13章：循环神经网路</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#13-1-基本结构"><span class="toc-text">13.1 基本结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#13-2-梯度计算"><span class="toc-text">13.2 梯度计算</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#13-3-应用实例"><span class="toc-text">13.3 应用实例</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#13-3-1-数据准备"><span class="toc-text">13.3.1 数据准备</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#13-3-2-模型构建"><span class="toc-text">13.3.2 模型构建</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#13-4-总结"><span class="toc-text">13.4 总结</span></a></li></ol></li></ol>
  </div>


  </div>
</div>
<div class="copyright">
    <span>本作品采用</span>
    <a href="https://creativecommons.org/licenses/by-nc-nd/4.0/deed.zh" target="_blank" rel="noopener">署名-非商业性使用-禁止演绎 4.0 国际许可协议</a>
    <span>进行许可。转载时请注明原文链接。</span>
</div>


  
    <div class="post-nav">
      <div class="post-nav-item post-nav-next">
        
          <span>〈 </span>
          <a href="/post/ai-0-ml-12/" rel="next" title="机器学习系列（十二）卷积神经网路">
          机器学习系列（十二）卷积神经网路
          </a>
        
      </div>
  
      <div class="post-nav-item post-nav-prev">
          
          <a href="/post/share-0-geobook-1/" rel="prev" title="分享一本郑颖人、龚晓南的 《岩土塑性力学基础》">
            分享一本郑颖人、龚晓南的 《岩土塑性力学基础》
          </a>
          <span>〉</span>
        
      </div>
    </div>
  



</div>
</div>

<footer class="footer text-center">
    <div id="bottom-inner">
        <a class="bottom-item" href="" target="_blank">QH的博客</a> |
        <a class="bottom-item">Powered by hexo</a> |
        <a class="bottom-item">Copyright © 2023</a>  |
        <a>博客已运行<a id="days">0</a>天</a> 
        <script> 
        var s1 = '2020-04-15';//设置为你的建站时间 
        s1 = new Date(s1.replace(/-/g, "/")); 
        s2 = new Date(); 
        var days = s2.getTime() - s1.getTime(); 
        var number_of_days = parseInt(days / (1000 * 60 * 60 * 24)); 
        document.getElementById('days').innerHTML = number_of_days; 
        </script> |
        <a class="bottom-item" href="/atom.xml">RSS</a>
        <script>
			var _hmt = _hmt || [];
			(function() {
			  var hm = document.createElement("script");
			  hm.src = "https://hm.baidu.com/hm.js?7241c66da4c19bc02e939e6776919ff8";
			  var s = document.getElementsByTagName("script")[0]; 
			  s.parentNode.insertBefore(hm, s);
			})();
		</script>
    </div>
</footer>



<script>
  (function(window, document, undefined) {

    var timer = null;

    function returnTop() {
      cancelAnimationFrame(timer);
      timer = requestAnimationFrame(function fn() {
        var oTop = document.body.scrollTop || document.documentElement.scrollTop;
        if (oTop > 0) {
          document.body.scrollTop = document.documentElement.scrollTop = oTop - 50;
          timer = requestAnimationFrame(fn);
        } else {
          cancelAnimationFrame(timer);
        }
      });
    }

    var hearts = [];
    window.requestAnimationFrame = (function() {
      return window.requestAnimationFrame ||
        window.webkitRequestAnimationFrame ||
        window.mozRequestAnimationFrame ||
        window.oRequestAnimationFrame ||
        window.msRequestAnimationFrame ||
        function(callback) {
          setTimeout(callback, 1000 / 60);
        }
    })();
    init();

    function init() {
      css(".heart{z-index:9999;width: 10px;height: 10px;position: fixed;background: #f00;transform: rotate(45deg);-webkit-transform: rotate(45deg);-moz-transform: rotate(45deg);}.heart:after,.heart:before{content: '';width: inherit;height: inherit;background: inherit;border-radius: 50%;-webkit-border-radius: 50%;-moz-border-radius: 50%;position: absolute;}.heart:after{top: -5px;}.heart:before{left: -5px;}");
      attachEvent();
      gameloop();
      addMenuEvent();
    }

    function gameloop() {
      for (var i = 0; i < hearts.length; i++) {
        if (hearts[i].alpha <= 0) {
          document.body.removeChild(hearts[i].el);
          hearts.splice(i, 1);
          continue;
        }
        hearts[i].y--;
        hearts[i].scale += 0.004;
        hearts[i].alpha -= 0.013;
        hearts[i].el.style.cssText = "left:" + hearts[i].x + "px;top:" + hearts[i].y + "px;opacity:" + hearts[i].alpha + ";transform:scale(" + hearts[i].scale + "," + hearts[i].scale + ") rotate(45deg);background:" + hearts[i].color;
      }
      requestAnimationFrame(gameloop);
    }

    /**
     * 给logo设置点击事件
     * 
     * - 回到顶部
     * - 出现爱心
     */
    function attachEvent() {
      var old = typeof window.onclick === "function" && window.onclick;
      var logo = document.getElementById("logo");
      if (logo) {
        logo.onclick = function(event) {
          returnTop();
          old && old();
          createHeart(event);
        }
      }
      
    }

    function createHeart(event) {
      var d = document.createElement("div");
      d.className = "heart";
      hearts.push({
        el: d,
        x: event.clientX - 5,
        y: event.clientY - 5,
        scale: 1,
        alpha: 1,
        color: randomColor()
      });
      document.body.appendChild(d);
    }

    function css(css) {
      var style = document.createElement("style");
      style.type = "text/css";
      try {
        style.appendChild(document.createTextNode(css));
      } catch (ex) {
        style.styleSheet.cssText = css;
      }
      document.getElementsByTagName('head')[0].appendChild(style);
    }

    function randomColor() {
      // return "rgb(" + (~~(Math.random() * 255)) + "," + (~~(Math.random() * 255)) + "," + (~~(Math.random() * 255)) + ")";
      return "#F44336";
    }

    function addMenuEvent() {
      var menu = document.getElementById('menu-main-post');
      if (menu) {
        var toc = document.getElementById('toc');
        if (toc) {
          menu.onclick = function() {
            if (toc) {
              if (toc.style.display == 'block') {
                toc.style.display = 'none';
              } else {
                toc.style.display = 'block';
              }
            }
          };
        } else {
          menu.style.display = 'none';
        }
      }
    }

  })(window, document);
</script>






<script>
  (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      if (curProtocol === 'https') {
          bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
      }
      else {
          bp.src = 'http://push.zhanzhang.baidu.com/push.js';
      }
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
  })();
</script>


<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
</body>
</html>